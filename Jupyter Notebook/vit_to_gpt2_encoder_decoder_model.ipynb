{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-10-27T23:47:25.577784Z",
     "iopub.status.busy": "2022-10-27T23:47:25.577170Z",
     "iopub.status.idle": "2022-10-27T23:48:10.508324Z",
     "shell.execute_reply": "2022-10-27T23:48:10.507465Z",
     "shell.execute_reply.started": "2022-10-27T23:47:25.577686Z"
    },
    "id": "XX9MrzCGw-K3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x2a5b23a60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.datasets as dset\n",
    "import random\n",
    "from transformers import EncoderDecoderModel, GPT2Tokenizer, ViTFeatureExtractor\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torchvision.datasets as dset\n",
    "import multiprocessing as mp\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pytorch_lightning as pl\n",
    "from deepspeed.ops.adam import FusedAdam\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "# import wandb\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7IaFK47w-K5"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T00:07:50.875886Z",
     "iopub.status.busy": "2021-12-27T00:07:50.875635Z",
     "iopub.status.idle": "2021-12-27T00:07:50.883294Z",
     "shell.execute_reply": "2021-12-27T00:07:50.882656Z",
     "shell.execute_reply.started": "2021-12-27T00:07:50.87585Z"
    },
    "id": "_1xy-BN5w-K6"
   },
   "outputs": [],
   "source": [
    "VIT_MODEL = \"google/vit-base-patch16-224-in21k\"\n",
    "GPT2 = \"gpt2\"\n",
    "DISTIL_GPT2 = \"distilgpt2\"\n",
    "\n",
    "DATA_PATH = \"Processed_Frames/\"\n",
    "\n",
    "# \"/content/drive/MyDrive/CS640 Project/Processed Frames/\"\n",
    "ANNOTATION_PATH = \"/content/drive/MyDrive/CS640 Project/Y.json\"\n",
    "\n",
    "MEAN = (0.485, 0.456, 0.406)\n",
    "STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "TRAIN_PCT = 0.95\n",
    "NUM_WORKERS = mp.cpu_count()\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "LR = 1e-4\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "MAX_TEXT_LENGTH = 32\n",
    "\n",
    "LABEL_MASK = -100\n",
    "\n",
    "TOP_K = 1000\n",
    "TOP_P = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GNBwCXNuT7lw"
   },
   "outputs": [],
   "source": [
    "y = pd.read_csv('Y.csv')\n",
    "# y.to_json('/content/drive/MyDrive/CS640 Project/Y.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "lEw1B4fqrjmQ",
    "outputId": "7af79494-6771-4599-ae7e-ccb613c83d68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Caroline Ingalls is a ZOMBIE!! -- New subreddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Caroline Ingalls is a ZOMBIE!! -- New subreddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caroline Ingalls is a ZOMBIE!! -- New subreddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Caroline Ingalls is a ZOMBIE!! -- New subreddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Caroline Ingalls is a ZOMBIE!! -- New subreddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049</th>\n",
       "      <td>Elpedroym0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2050</th>\n",
       "      <td>Elpedroym1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>Elpedroym2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>Elpedroym3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>FRIENDS WITH BENEFITS..0.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2054 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  label\n",
       "0     Caroline Ingalls is a ZOMBIE!! -- New subreddi...\n",
       "1     Caroline Ingalls is a ZOMBIE!! -- New subreddi...\n",
       "2     Caroline Ingalls is a ZOMBIE!! -- New subreddi...\n",
       "3     Caroline Ingalls is a ZOMBIE!! -- New subreddi...\n",
       "4     Caroline Ingalls is a ZOMBIE!! -- New subreddi...\n",
       "...                                                 ...\n",
       "2049                                     Elpedroym0.png\n",
       "2050                                     Elpedroym1.png\n",
       "2051                                     Elpedroym2.png\n",
       "2052                                     Elpedroym3.png\n",
       "2053                       FRIENDS WITH BENEFITS..0.png\n",
       "\n",
       "[2054 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8Qe5NYGw-K7"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "XOCpsYq80-yv"
   },
   "outputs": [],
   "source": [
    "class GifDataset(Dataset):\n",
    "    \"\"\"Gif Caption dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(idx)\n",
    "        print(self.labels.iloc[idx, 0])\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.labels.iloc[idx, 0])\n",
    "        print(self.labels.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        labels = self.labels.iloc[idx, 0]\n",
    "        labels = np.array([labels])\n",
    "        \n",
    "        # labels = labels.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'labels': labels, 'idx': idx}\n",
    "        \n",
    "        if self.transform:\n",
    "            # print(sample['labels'].shape)\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "j9wVVRAmaWcx"
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, idx = sample['image'], sample['idx']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        # landmarks = landmarks * [new_w / w, new_h / h]\n",
    "        # print(landmarks)\n",
    "\n",
    "        return {'image': img, 'idx': idx}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, idx = sample['image'], sample['idx']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        # print(landmarks)\n",
    "        # return {'image': torch.from_numpy(image),\n",
    "        #         'labels': landmarks}\n",
    "        # print(landmarks)\n",
    "        return {'image': torch.from_numpy(image), 'idx': idx}\n",
    "\n",
    "def show_gifs(image, labels):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(labels[:, 0], labels[:, 1], s=10, marker='.', c='r')\n",
    "    plt.pause(0.001)\n",
    "\n",
    "\n",
    "scale = Rescale(512)\n",
    "composed = transforms.Compose([Rescale((224,224))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "NQqY_CU4crOF"
   },
   "outputs": [],
   "source": [
    "image_path = 'Processed_Frames/'\n",
    "gif_data = GifDataset(csv_file='Y.csv',\n",
    "                                    root_dir= image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "3cGmDAYqg8wy"
   },
   "outputs": [],
   "source": [
    "transformed_dataset = GifDataset(csv_file='Y.csv',\n",
    "                                           root_dir=image_path,\n",
    "                                           transform=transforms.Compose([\n",
    "                                               ToTensor()\n",
    "                                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T00:07:54.398703Z",
     "iopub.status.busy": "2021-12-27T00:07:54.398451Z",
     "iopub.status.idle": "2021-12-27T00:07:57.898175Z",
     "shell.execute_reply": "2021-12-27T00:07:57.897421Z",
     "shell.execute_reply.started": "2021-12-27T00:07:54.39867Z"
    },
    "id": "3QAzgwTdw-K8"
   },
   "outputs": [],
   "source": [
    "train_len = int(TRAIN_PCT * len(transformed_dataset))\n",
    "train_data, valid_data = random_split(transformed_dataset, [train_len, len(transformed_dataset) - train_len])\n",
    "train_dl = DataLoader(\n",
    "    train_data, \n",
    "    BATCH_SIZE, \n",
    "    pin_memory=True, \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    drop_last=True\n",
    ")\n",
    "valid_dl = DataLoader(\n",
    "    valid_data, \n",
    "    BATCH_SIZE, \n",
    "    pin_memory=True, \n",
    "    shuffle=False, \n",
    "    num_workers=0, \n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# # images, captions = next(iter(train_dl))\n",
    "# images, idx = next(iter(train_dl))\n",
    "# images = images\n",
    "# images.shape, images.min(), images.max(), images.mean(), images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "id": "a8urz_dnutEY",
    "outputId": "9005bed9-fb3f-4019-c49c-ec54dcf96468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1106\n",
      "When Elmo Is Talking You Shut Your Trap37.png\n",
      "When Elmo Is Talking You Shut Your Trap37.png\n",
      "130\n",
      "Neighbor kids causing mischief...47.png\n",
      "Neighbor kids causing mischief...47.png\n",
      "1153\n",
      "When Elmo Is Talking You Shut Your Trap84.png\n",
      "When Elmo Is Talking You Shut Your Trap84.png\n",
      "1863\n",
      "Funny Pasta16.png\n",
      "Funny Pasta16.png\n",
      "1116\n",
      "When Elmo Is Talking You Shut Your Trap47.png\n",
      "When Elmo Is Talking You Shut Your Trap47.png\n",
      "1449\n",
      "A Murder Knife Mystery15.png\n",
      "A Murder Knife Mystery15.png\n",
      "1964\n",
      "The end of the line for Microsoft Customer Support.0.png\n",
      "The end of the line for Microsoft Customer Support.0.png\n",
      "1349\n",
      "Let’s set off a fire cracker near some open windows7.png\n",
      "Let’s set off a fire cracker near some open windows7.png\n",
      "1499\n",
      "gottem6.png\n",
      "gottem6.png\n",
      "837\n",
      "Skte17.png\n",
      "Skte17.png\n",
      "1043\n",
      "Iran coming up..2.png\n",
      "Iran coming up..2.png\n",
      "1261\n",
      "Fly Me To The Moon17.png\n",
      "Fly Me To The Moon17.png\n",
      "1457\n",
      "Not giving AF0.png\n",
      "Not giving AF0.png\n",
      "1285\n",
      "Fly Me To The Moon41.png\n",
      "Fly Me To The Moon41.png\n",
      "1219\n",
      "When you got pussy on your mind but you're stuck in court2.png\n",
      "When you got pussy on your mind but you're stuck in court2.png\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: '/Users/aakashbhatnagar/Documents/masters/CS 640 AI/Project/Processed_Frames/When you got pussy on your mind but you're stuck in court2.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_dl:\n\u001b[1;32m      2\u001b[0m     images \u001b[38;5;241m=\u001b[39m i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m     idx \u001b[38;5;241m=\u001b[39m i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn [56], line 29\u001b[0m, in \u001b[0;36mGifDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m img_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir,\n\u001b[1;32m     26\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 29\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([labels])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/skimage/io/_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtifffile\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_or_url_context(fname) \u001b[38;5;28;01mas\u001b[39;00m fname:\n\u001b[0;32m---> 53\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcall_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimread\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mplugin_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/skimage/io/manage_plugins.py:207\u001b[0m, in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find the plugin \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    205\u001b[0m                            (plugin, kind))\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/skimage/io/_plugins/imageio_plugin.py:15\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(imageio_imread)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mimageio_imread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/imageio/v2.py:226\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m imopen_args \u001b[38;5;241m=\u001b[39m decypher_format_arg(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    224\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimopen_args\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    227\u001b[0m     result \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/imageio/core/imopen.py:118\u001b[0m, in \u001b[0;36mimopen\u001b[0;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     request\u001b[38;5;241m.\u001b[39mformat_hint \u001b[38;5;241m=\u001b[39m format_hint\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<bytes>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(uri, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m uri\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# fast-path based on plugin\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# (except in legacy mode)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/imageio/core/request.py:248\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[0;34m(self, uri, mode, extension, format_hint, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Request.Mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Parse what was given\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# Set extension\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extension \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/imageio/core/request.py:407\u001b[0m, in \u001b[0;36mRequest._parse_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_read_request:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Reading: check that the file exists (but is allowed a dir)\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(fn):\n\u001b[0;32m--> 407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m fn)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# Writing: check that the directory to write to does exist\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     dn \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(fn)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file: '/Users/aakashbhatnagar/Documents/masters/CS 640 AI/Project/Processed_Frames/When you got pussy on your mind but you're stuck in court2.png'"
     ]
    }
   ],
   "source": [
    "for i in train_dl:\n",
    "    images = i['image']\n",
    "    idx = i['idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>If the veggies can do it then why can’t you.3.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  label\n",
       "1416  If the veggies can do it then why can’t you.3.png"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[y['label'] == 'If the veggies can do it then why can’t you.3.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "6mKLtd1sx9Gp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2054"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNlfXV_krlw_"
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in transformed_dataset:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KayXd7alm23j",
    "outputId": "22083353-f482-4c36-c6c9-97c551196b10"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'idx'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "XEqkMIJerL6L",
    "outputId": "2a48f845-614c-4ce2-c868-3a6f19c9e85c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = {'a':[1,2], 'b': [4,5]}\n",
    "next(iter(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1HSmz3SqMfC",
    "outputId": "c61b409c-069e-4ac4-8204-efbf0e26aa61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neighbor kids causing mischief...136.png'], dtype='<U40')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(['Neighbor kids causing mischief...136.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k_rDpFOw-K8"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T00:07:57.900281Z",
     "iopub.status.busy": "2021-12-27T00:07:57.899853Z",
     "iopub.status.idle": "2021-12-27T00:08:30.49721Z",
     "shell.execute_reply": "2021-12-27T00:08:30.49646Z",
     "shell.execute_reply.started": "2021-12-27T00:07:57.900241Z"
    },
    "id": "X0b1Jh71w-K9"
   },
   "outputs": [],
   "source": [
    "# make sure GPT2 appends EOS in begin and end\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "    \n",
    "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(DISTIL_GPT2)\n",
    "# set pad_token_id to unk_token_id -> be careful here as unk_token_id == eos_token_id == bos_token_id\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token\n",
    "\n",
    "gpt2_tokenizer_fn = lambda x: gpt2_tokenizer(\n",
    "    x,\n",
    "    max_length=MAX_TEXT_LENGTH,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "vit2gpt2 = EncoderDecoderModel.from_encoder_decoder_pretrained(VIT_MODEL, DISTIL_GPT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JP-egZFw-K9"
   },
   "source": [
    "## Nucleus Sampling\n",
    "[Paper](https://arxiv.org/pdf/1904.09751.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T00:24:51.921354Z",
     "iopub.status.busy": "2021-12-27T00:24:51.921088Z",
     "iopub.status.idle": "2021-12-27T00:24:51.93938Z",
     "shell.execute_reply": "2021-12-27T00:24:51.938484Z",
     "shell.execute_reply.started": "2021-12-27T00:24:51.921325Z"
    },
    "id": "bPlfMxLow-K9"
   },
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(\n",
    "    next_token_logits: torch.FloatTensor,\n",
    "    top_k: Optional[float]=None, \n",
    "    top_p: Optional[float]=None,\n",
    "    device: Union[str, torch.device]=\"cpu\",\n",
    ") -> torch.FloatTensor:\n",
    "    if top_k is None:\n",
    "        top_k = next_token_logits.shape[-1]\n",
    "    if top_p is None:\n",
    "        top_p = 1.0\n",
    "        \n",
    "    p, largest_p_idx = F.softmax(next_token_logits, dim=-1).topk(top_k, dim=-1)\n",
    "    cumulative_p = p.cumsum(dim=-1)\n",
    "    threshold_repeated = top_p + torch.zeros((len(p),1)).to(device)\n",
    "    idx = torch.searchsorted(cumulative_p, threshold_repeated).clip(max=top_k-1).squeeze()\n",
    "    cutoffs = cumulative_p[torch.arange(len(cumulative_p)), idx]\n",
    "    censored_p = (cumulative_p <= cutoffs[:, None]) * p\n",
    "    renormalized_p = censored_p / censored_p.sum(dim=-1, keepdims=True)\n",
    "    \n",
    "    final_p = torch.zeros_like(next_token_logits)\n",
    "    row_idx = torch.arange(len(p)).unsqueeze(1).repeat(1,top_k).to(device)\n",
    "    final_p[row_idx, largest_p_idx] = renormalized_p.to(final_p.dtype)\n",
    "\n",
    "    return final_p\n",
    "\n",
    "def generate_sentence_from_image(model, encoder_outputs, tokenizer, max_text_length: int, device)-> List[str]:\n",
    "    generated_so_far = torch.LongTensor([[tokenizer.bos_token_id]]*len(encoder_outputs.last_hidden_state)).to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(max_text_length)):\n",
    "            attention_mask = torch.ones_like(generated_so_far)\n",
    "            decoder_out = model(\n",
    "                decoder_input_ids=generated_so_far, \n",
    "                decoder_attention_mask=attention_mask,\n",
    "                encoder_outputs=encoder_outputs\n",
    "            )\n",
    "\n",
    "            next_token_logits = decoder_out[\"logits\"][:, -1, :]\n",
    "            filtered_p = top_k_top_p_filtering(next_token_logits, top_k=TOP_K, top_p=TOP_P, device=device)\n",
    "            next_token = torch.multinomial(filtered_p, num_samples=1)\n",
    "            generated_so_far = torch.cat((generated_so_far, next_token), dim=1)\n",
    "\n",
    "    return [tokenizer.decode(coded_sentence) for coded_sentence in generated_so_far]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OLD0lAZw-K-"
   },
   "source": [
    "## Training Module (PyTorch Lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T00:25:24.081999Z",
     "iopub.status.busy": "2021-12-27T00:25:24.081677Z",
     "iopub.status.idle": "2021-12-27T00:25:24.104356Z",
     "shell.execute_reply": "2021-12-27T00:25:24.103482Z",
     "shell.execute_reply.started": "2021-12-27T00:25:24.081963Z"
    },
    "id": "MNlT5XFRw-K-"
   },
   "outputs": [],
   "source": [
    "class LightningModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        tokenizer,\n",
    "        lr: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lr = lr\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"crossattention\" not in name:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def common_step(self, batch: Tuple[torch.FloatTensor, List[str]]) -> torch.FloatTensor:\n",
    "        images, captions = batch\n",
    "        tokenized_captions = {\n",
    "            k: v.to(self.device) for k, v in \n",
    "            self.tokenizer(\n",
    "                captions,\n",
    "                max_length=MAX_TEXT_LENGTH,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).items()\n",
    "        }\n",
    "        labels = tokenized_captions[\"input_ids\"].clone()\n",
    "        labels[tokenized_captions[\"attention_mask\"]==0] = LABEL_MASK\n",
    "        encoder_outputs = self.model.encoder(pixel_values=images)\n",
    "        outputs = self.model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            decoder_input_ids=tokenized_captions[\"input_ids\"],\n",
    "            decoder_attention_mask=tokenized_captions[\"attention_mask\"],\n",
    "            labels=labels,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        return outputs[\"loss\"]\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.FloatTensor, List[str]], batch_idx: int) -> torch.FloatTensor:\n",
    "        loss = self.common_step(batch)\n",
    "        self.log(name=\"Training loss\", value=loss, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch: Tuple[torch.FloatTensor, List[str]], batch_idx: int):\n",
    "        loss = self.common_step(batch)\n",
    "        self.log(name=\"Validation loss\", value=loss, on_step=True, on_epoch=True)\n",
    "\n",
    "        images, actual_sentences = batch\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            encoder_outputs = self.model.encoder(pixel_values=images.to(self.device))\n",
    "            generated_sentences = generate_sentence_from_image(\n",
    "                self.model, \n",
    "                encoder_outputs, \n",
    "                self.tokenizer, \n",
    "                MAX_TEXT_LENGTH,\n",
    "                self.device\n",
    "            )\n",
    "            images = [wandb.Image(transforms.ToPILImage()(descale(image))) for image in images]\n",
    "            data = list(map(list, zip(images, actual_sentences, generated_sentences)))\n",
    "            columns = [\"Images\", \"Actual Sentence\", \"Generated Sentence\"]\n",
    "            table = wandb.Table(data=data, columns=columns)\n",
    "            self.logger.experiment.log({f\"epoch {self.current_epoch} results\": table})\n",
    "                        \n",
    "    def on_after_backward(self):\n",
    "        if self.trainer.global_step % 50 == 0:  # don't make the tf file huge\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if \"weight\" in name and not \"norm\" in name and param.requires_grad:\n",
    "                    self.logger.experiment.log(\n",
    "                        {f\"{name}_grad\": wandb.Histogram(param.grad.detach().cpu())}\n",
    "                    )\n",
    "                    self.logger.experiment.log(\n",
    "                        {f\"{name}\": wandb.Histogram(param.detach().cpu())}\n",
    "                    )\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T00:25:27.303776Z",
     "iopub.status.busy": "2021-12-27T00:25:27.303497Z",
     "iopub.status.idle": "2021-12-27T00:25:36.522446Z",
     "shell.execute_reply": "2021-12-27T00:25:36.521481Z",
     "shell.execute_reply.started": "2021-12-27T00:25:27.303745Z"
    },
    "id": "QMnscejQw-K-"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /kaggle/working/logs\n",
    "lightning_module = LightningModule(\n",
    "    vit2gpt2,\n",
    "    gpt2_tokenizer,\n",
    "    LR\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    gpus=torch.cuda.device_count(),\n",
    "    gradient_clip_val=1.0,\n",
    "    logger=WandbLogger(\"Frozen\", \"/kaggle/working/logs/\", project=\"Vit2GPT2\"),\n",
    "    precision=16,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "trainer.fit(lightning_module, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--mpg97Nw-K-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
